{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lead\n",
    "from pyspark.sql.types import (IntegerType, LongType)\n",
    "from pyspark.sql.functions import (month, year)\n",
    "from pyspark.sql.functions import udf, from_unixtime, unix_timestamp\n",
    "#from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estado_actividad(u_id, nxt_u_id, per, nxt_per):\n",
    "    \n",
    "    if u_id == nxt_u_id and 12*(nxt_per.year - per.year) + (nxt_per.month - per.month) >= 4:#12*(year(nxt_per) - year(per)) + (month(nxt_per) - month(per)) >= 4:\n",
    "        \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Preprocessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_act = spark.read.parquet('activityObfmod5.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_act_int = raw_data_act.withColumn('user_id', raw_data_act['user_id'].cast(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act_wo_duplicates = raw_data_act_int.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_act_wo_duplicates.write.csv(\"act_wo_duplicates.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act_wo_duplicates = spark.read.csv(\"act_wo_duplicates.csv/\", header = True, inferSchema = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_act_ordered = df_act_wo_duplicates.orderBy(['user_id', 'periodo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### CREO COLUMNAS next_user_id Y next_periodo\n",
    "\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window.orderBy(['user_id','periodo']) ## --> esto ordena por user_id y periodo\n",
    "\n",
    "df_act_orderer_w_lead = df_act_wo_duplicates.withColumn(\"next_periodo\", lead(df_act_wo_duplicates.periodo).over(w))\\\n",
    "            .withColumn(\"next_user_id\", lead(df_act_wo_duplicates.user_id).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Para merge de df's de act. y sb, ver --> https://stackoverflow.com/questions/38063657/pyspark-merge-outer-join-two-data-frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "define_estados = udf(estado_actividad, IntegerType())    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act_final = df_act_orderer_w_lead.withColumn(\"estado\", define_estados(df_act_orderer_w_lead.user_id,\n",
    "                                                         df_act_orderer_w_lead.next_user_id,\n",
    "                                                         df_act_orderer_w_lead.periodo,\n",
    "                                                         df_act_orderer_w_lead.next_periodo)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act_final.select(['user_id','periodo','estado']).write.csv(\"df_act_final.csv\", header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe act con estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act_final= spark.read.csv('df_act_final.csv/', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_pattern = 'yyyy-MM-dd hh:mm:ss'\n",
    "to_pattern = 'yyyy-MM-dd'\n",
    "\n",
    "df_act_final = df_act_final.withColumn(\"periodo\", from_unixtime(unix_timestamp(df_act_final.periodo, \n",
    "            from_pattern), to_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_act_final.select(['user_id','periodo','estado']).show()\n",
    "df_act_final.select('periodo').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb = spark.read.parquet('sbObfmod5.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sb.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_int = df_sb.withColumn('user_id', df_sb['user_id'].cast(LongType()))\n",
    "df_sb_int.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sb_wo_duplicates = df_sb_int.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_int.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sb_int.cache()\n",
    "# df_act_final.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hago un 'join' entre los data frames de actividad y features, según user_id y período"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_act = df_sb_int.orderBy(['user_id', 'periodo']).join(df_act_final.orderBy(['user_id', 'periodo']),\n",
    "                                                           on = ['user_id', 'periodo'], how='inner' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sb_act.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sb_act.filter(df_sb_act.estado == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sb_act.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sb_act.dropDuplicates().filter(df_sb_act.estado == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_act.dropDuplicates().write.csv('dataFrame_actividad_features.csv', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_sb_act es el dataframe con los features de cada usuario + esado de actividad (1--> se fuga, 0--> no se fuga). \n",
    "### Después de sacarse de encima las filas duplicadas, la cantidad filas con estado = 1 es 23.665, mientras que la cantidad total de filas es 6.897.273\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_act = spark.read.csv('dataFrame_actividad_features.csv/', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'periodo',\n",
       " 'fecha_proceso',\n",
       " 'R04PERREF',\n",
       " 'total_deudores_informados',\n",
       " 'deuda_vigente',\n",
       " 'deuda_directa_morosa90',\n",
       " 'deuda_directa_vencida',\n",
       " 'deuda_directa_mora180',\n",
       " 'deuda_indirecta_mora180',\n",
       " 'deuda_directa_inversiones_financieras',\n",
       " 'deuda_directa_ops_pacto',\n",
       " 'deuda_indirecta_vigente',\n",
       " 'deuda_indirecta_vencida',\n",
       " 'deuda_directa_comercial',\n",
       " 'deuda_directa_cred_consumo',\n",
       " 'deuda_directa_hipotecaria',\n",
       " 'deuda_directa_comercial_ext',\n",
       " 'deuda_indirecta_comercial_ext',\n",
       " 'deuda_directa_leasing',\n",
       " 'deuda_morosa_leasing',\n",
       " 'monto_lineas_cred_disp',\n",
       " 'estado']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sb_act.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_act_ordered = df_sb_act.orderBy(['estado', 'periodo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_estado_mes_anterior(estado, next_estado, next_2_estado, next_3_estado, \n",
    "                               user_id, next_user_id, next_2user_id, next_3user_id):\n",
    "    \n",
    "    if (user_id == next_user_id) and (estado == 0 and next_estado == 1):\n",
    "        return 1\n",
    "    elif (user_id == next_2user_id) and (estado == 0 and next_2_estado == 1):\n",
    "        return 1\n",
    "    elif (user_id == next_3user_id) and (estado == 0 and next_3_estado == 1):\n",
    "        return 1    \n",
    "    elif estado == 1:  #\n",
    "        return 0\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window.orderBy(['user_id','periodo']) ## --> esto ordena por user_id y periodo\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    '''Para cada usuario: si estado = 1, asigna 1 al estado en los tres períodos (meses) anteriores '''\n",
    "\n",
    "define_estados_2 = udf(define_estado_mes_anterior, IntegerType())    \n",
    "\n",
    "df_sb_act_final = df_sb_act_ordered.withColumn('estado', define_estados_2(df_sb_act_ordered.estado,\n",
    "                                                           lead(df_sb_act_ordered.estado,1).over(w),\n",
    "                                                           lead(df_sb_act_ordered.estado,2).over(w),\n",
    "                                                           lead(df_sb_act_ordered.estado,3).over(w),\n",
    "                                                                          df_sb_act_ordered.user_id,\n",
    "                                                           lead(df_sb_act_ordered.user_id,1).over(w),\n",
    "                                                           lead(df_sb_act_ordered.user_id,2).over(w),\n",
    "                                                           lead(df_sb_act_ordered.user_id,3).over(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38146"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sb_act_final.filter('estado = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47330"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sb_act.filter('estado=1').count()*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_and_features = df_sb_act_final.select(['estado',\n",
    "                                             'deuda_vigente',\n",
    "                                             'deuda_directa_morosa90',\n",
    "                                             'deuda_directa_vencida',\n",
    "                                             'deuda_directa_mora180',\n",
    "                                             'deuda_indirecta_mora180',\n",
    "                                             'deuda_directa_inversiones_financieras',\n",
    "                                             'deuda_directa_ops_pacto',\n",
    "                                             'deuda_indirecta_vigente',\n",
    "                                             'deuda_indirecta_vencida',\n",
    "                                             'deuda_directa_comercial',\n",
    "                                             'deuda_directa_cred_consumo',\n",
    "                                             'deuda_directa_hipotecaria',\n",
    "                                             'deuda_directa_comercial_ext',\n",
    "                                             'deuda_indirecta_comercial_ext',\n",
    "                                             'deuda_directa_leasing',\n",
    "                                             'deuda_morosa_leasing',\n",
    "                                             'monto_lineas_cred_disp',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49105"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_and_features.filter('estado=1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+-------------------+\n",
      "|         user_id|estado|            periodo|\n",
      "+----------------+------+-------------------+\n",
      "| 564873096719398|     1|2016-01-01 00:00:00|\n",
      "| 564873096719398|     1|2016-02-01 00:00:00|\n",
      "| 564873096719398|     1|2016-03-01 00:00:00|\n",
      "| 564873096719398|     1|2016-04-01 00:00:00|\n",
      "| 785830050894577|     1|2016-07-01 00:00:00|\n",
      "| 785830050894577|     1|2016-08-01 00:00:00|\n",
      "| 785830050894577|     1|2016-09-01 00:00:00|\n",
      "| 785830050894577|     1|2016-10-01 00:00:00|\n",
      "|1053623958002368|     1|2018-04-01 00:00:00|\n",
      "|1053623958002368|     1|2018-05-01 00:00:00|\n",
      "|1053623958002368|     1|2018-06-01 00:00:00|\n",
      "|1053623958002368|     1|2018-07-01 00:00:00|\n",
      "|1158743260739798|     1|2016-06-01 00:00:00|\n",
      "|1158743260739798|     1|2016-07-01 00:00:00|\n",
      "|1158743260739798|     1|2016-08-01 00:00:00|\n",
      "|1158743260739798|     1|2016-09-01 00:00:00|\n",
      "|1256313568775196|     1|2016-07-01 00:00:00|\n",
      "|1256313568775196|     1|2016-08-01 00:00:00|\n",
      "|1256313568775196|     1|2016-09-01 00:00:00|\n",
      "|1256313568775196|     1|2016-10-01 00:00:00|\n",
      "|1284799256901122|     1|2015-11-01 00:00:00|\n",
      "|1284799256901122|     1|2015-12-01 00:00:00|\n",
      "|1284799256901122|     1|2016-01-01 00:00:00|\n",
      "|1284799256901122|     1|2016-02-01 00:00:00|\n",
      "|1386792560565804|     1|2016-02-01 00:00:00|\n",
      "|1386792560565804|     1|2016-03-01 00:00:00|\n",
      "|1386792560565804|     1|2016-04-01 00:00:00|\n",
      "|1386792560565804|     1|2016-05-01 00:00:00|\n",
      "|1431401262450598|     1|2016-04-01 00:00:00|\n",
      "|1431401262450598|     1|2016-05-01 00:00:00|\n",
      "|1431401262450598|     1|2016-06-01 00:00:00|\n",
      "|1431401262450598|     1|2016-07-01 00:00:00|\n",
      "|1445900724319576|     1|2013-02-01 00:00:00|\n",
      "|1555145271592322|     1|2014-01-01 00:00:00|\n",
      "|1606666043353156|     1|2014-04-01 00:00:00|\n",
      "|1816773851144037|     1|2016-01-01 00:00:00|\n",
      "|1816773851144037|     1|2016-02-01 00:00:00|\n",
      "|1816773851144037|     1|2016-03-01 00:00:00|\n",
      "|1816773851144037|     1|2016-04-01 00:00:00|\n",
      "|1902939560112407|     1|2014-05-01 00:00:00|\n",
      "|2004265258087266|     1|2016-05-01 00:00:00|\n",
      "|2004265258087266|     1|2016-06-01 00:00:00|\n",
      "|2004265258087266|     1|2016-07-01 00:00:00|\n",
      "|2004265258087266|     1|2016-08-01 00:00:00|\n",
      "|2005997323504851|     1|2016-04-01 00:00:00|\n",
      "|2005997323504851|     1|2016-05-01 00:00:00|\n",
      "|2005997323504851|     1|2016-06-01 00:00:00|\n",
      "|2005997323504851|     1|2016-07-01 00:00:00|\n",
      "|2033942835317759|     1|2016-06-01 00:00:00|\n",
      "|2033942835317759|     1|2016-07-01 00:00:00|\n",
      "|2033942835317759|     1|2016-08-01 00:00:00|\n",
      "|2033942835317759|     1|2016-09-01 00:00:00|\n",
      "|2038585462850291|     1|2014-07-01 00:00:00|\n",
      "|2047978673657348|     1|2016-04-01 00:00:00|\n",
      "|2047978673657348|     1|2016-05-01 00:00:00|\n",
      "|2047978673657348|     1|2016-06-01 00:00:00|\n",
      "|2047978673657348|     1|2016-07-01 00:00:00|\n",
      "|2078799731444990|     1|2014-07-01 00:00:00|\n",
      "|2150636455112977|     1|2014-05-01 00:00:00|\n",
      "|2225031268795353|     1|2017-02-01 00:00:00|\n",
      "|2225031268795353|     1|2017-03-01 00:00:00|\n",
      "|2225031268795353|     1|2017-04-01 00:00:00|\n",
      "|2225031268795353|     1|2017-05-01 00:00:00|\n",
      "|2285423263394692|     1|2016-08-01 00:00:00|\n",
      "|2285423263394692|     1|2016-09-01 00:00:00|\n",
      "|2285423263394692|     1|2016-10-01 00:00:00|\n",
      "|2285423263394692|     1|2016-11-01 00:00:00|\n",
      "|2295351254280339|     1|2016-06-01 00:00:00|\n",
      "|2295351254280339|     1|2016-07-01 00:00:00|\n",
      "|2295351254280339|     1|2016-08-01 00:00:00|\n",
      "|2295351254280339|     1|2016-09-01 00:00:00|\n",
      "|2334256970782193|     1|2016-04-01 00:00:00|\n",
      "|2334256970782193|     1|2016-05-01 00:00:00|\n",
      "|2334256970782193|     1|2016-06-01 00:00:00|\n",
      "|2334256970782193|     1|2016-07-01 00:00:00|\n",
      "|2339684728106301|     1|2014-06-01 00:00:00|\n",
      "|2355317039897161|     1|2014-04-01 00:00:00|\n",
      "|2362797031322458|     1|2017-11-01 00:00:00|\n",
      "|2362797031322458|     1|2017-12-01 00:00:00|\n",
      "|2362797031322458|     1|2018-01-01 00:00:00|\n",
      "|2362797031322458|     1|2018-02-01 00:00:00|\n",
      "|2446691279910464|     1|2016-06-01 00:00:00|\n",
      "|2446691279910464|     1|2016-07-01 00:00:00|\n",
      "|2446691279910464|     1|2016-08-01 00:00:00|\n",
      "|2446691279910464|     1|2016-09-01 00:00:00|\n",
      "|2504569931022296|     1|2014-08-01 00:00:00|\n",
      "|2535450674369758|     1|2016-05-01 00:00:00|\n",
      "|2535450674369758|     1|2016-06-01 00:00:00|\n",
      "|2535450674369758|     1|2016-07-01 00:00:00|\n",
      "|2535450674369758|     1|2016-08-01 00:00:00|\n",
      "|2538563276645014|     1|2016-06-01 00:00:00|\n",
      "|2538563276645014|     1|2016-07-01 00:00:00|\n",
      "|2538563276645014|     1|2016-08-01 00:00:00|\n",
      "|2538563276645014|     1|2016-09-01 00:00:00|\n",
      "|2543809832202848|     1|2014-08-01 00:00:00|\n",
      "|2543809832202848|     1|2018-04-01 00:00:00|\n",
      "|2543809832202848|     1|2018-05-01 00:00:00|\n",
      "|2543809832202848|     1|2018-06-01 00:00:00|\n",
      "|2543809832202848|     1|2018-07-01 00:00:00|\n",
      "|2621803693588737|     1|2014-03-01 00:00:00|\n",
      "+----------------+------+-------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sb_act_final.filter('estado=1').select(['user_id','estado','periodo']).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_and_features.write.csv('dataFrame target and features(fuga 3 meses antes no-current).csv', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosas de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux1 = spark.createDataFrame([(1, 15, 1), (2,30,0), (3,43,1), (4,21,0), (5,344,0)], ['col1','col2', 'col3'])\n",
    "df_aux2 = spark.createDataFrame([(1, 15), (2,30), (3,43), (4,21), (7,89)], ['col1','col2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux1.select(['col1','col2']).union(df_aux2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|  15|   1|\n",
      "|   2|  30|   0|\n",
      "|   3|  43|   1|\n",
      "|   4|  21|   0|\n",
      "|   5| 344|   0|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aux1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----------+\n",
      "|col1|col2|col3|next_2_col2|\n",
      "+----+----+----+-----------+\n",
      "|   1|  15|   1|         30|\n",
      "|   4|  21|   0|         43|\n",
      "|   2|  30|   0|        344|\n",
      "|   3|  43|   1|       null|\n",
      "|   5| 344|   0|       null|\n",
      "+----+----+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aux1.withColumn('next_2_col2', lead(df_aux1.col2,2).over(Window.orderBy('col2'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_aux2.join(df_aux1, on=['col1', 'col2'], how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
